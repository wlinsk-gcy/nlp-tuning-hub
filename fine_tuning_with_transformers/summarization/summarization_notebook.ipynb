{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 安装依赖\n",
    "\n",
    "如果当前环境已经有相关依赖了，则不用执行"
   ],
   "id": "5e5db344fa1d02df"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "! pip install datasets==3.6.0 evaluate transformers rouge-score nltk",
   "id": "1f3e1fe652dcc35c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 加载数据\n",
    "\n",
    "本案例使用的是XSum数据集，其中包含BBC的文章以及单句的摘要等信息；数据集有点大，在本地下载时请注意磁盘空间；"
   ],
   "id": "ea7f2098a2681bc8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from datasets import load_dataset\n",
    "from evaluate import load\n",
    "\n",
    "model_checkpoint = \"t5-small\"\n",
    "\n",
    "raw_datasets = load_dataset(\"xsum\")\n",
    "metric = load(\"rouge\")"
   ],
   "id": "e84c7712ed695c7d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "你也可以看看datasets的数据结构长什么样：",
   "id": "8194c20b449d8cf4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "print(raw_datasets)",
   "id": "83b8c1a36f0674dd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "```json\n",
    "DatasetDict({\n",
    "    train: Dataset({\n",
    "        features: ['document', 'summary', 'id'],\n",
    "        num_rows: 204045\n",
    "    })\n",
    "    validation: Dataset({\n",
    "        features: ['document', 'summary', 'id'],\n",
    "        num_rows: 11332\n",
    "    })\n",
    "    test: Dataset({\n",
    "        features: ['document', 'summary', 'id'],\n",
    "        num_rows: 11334\n",
    "    })\n",
    "})\n",
    "```"
   ],
   "id": "52457184e7af991f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "可以看到训练集的数据有20w条；\n",
    "\n",
    "其中：document是文章，summary是对应的摘要，id即为数据id；"
   ],
   "id": "d8b698999a885352"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 数据集可视化\n",
    "\n",
    "为了能够进一步理解数据长什么样子，下面的函数将从数据集里随机选择几个例子进行展示；"
   ],
   "id": "6cf0b8b4d6287d3d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import datasets\n",
    "import random\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def show_random_elements(dataset, num_examples=5):\n",
    "    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n",
    "    picks = []\n",
    "    for _ in range(num_examples):\n",
    "        pick = random.randint(0, len(dataset)-1)\n",
    "        while pick in picks:\n",
    "            pick = random.randint(0, len(dataset)-1)\n",
    "        picks.append(pick)\n",
    "\n",
    "    df = pd.DataFrame(dataset[picks])\n",
    "    for column, typ in dataset.features.items():\n",
    "        if isinstance(typ, datasets.ClassLabel):\n",
    "            df[column] = df[column].transform(lambda i: typ.names[i])\n",
    "    display(HTML(df.to_html()))\n",
    "\n",
    "show_random_elements(raw_datasets[\"train\"])"
   ],
   "id": "8fbf65c6ebe02776"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 数据预处理\n",
    "\n",
    "在将这些数据样本输入模型之前，需要对样本进行预处理，使用分词器Tokenizer对样本进行编码（即将分词后的token转成词汇表中，每一个词对应的ID），然后转换为模型所需要的格式，即可；"
   ],
   "id": "ab0618dbf2c4755"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 加载分词器\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ],
   "id": "8e46dc6d16d34cc7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "如果你想看到分词后的句子是什么样子的话",
   "id": "96b02fc61f94661e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "tokenizer(\"Hello, this one sentence\")",
   "id": "3abf4ae0a430e7e9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "```json\n",
    "{'input_ids': [8774, 6, 48, 80, 7142, 1], 'attention_mask': [1, 1, 1, 1, 1, 1]}\n",
    "```"
   ],
   "id": "85568c68ea5633af"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "如果你使用的是T5系列模型，那么在做微调时，需要在样本头添加一个前缀，告诉LLM你在做什么任务；本案例是摘要生成，那么就加一个summarize即可；",
   "id": "4cb0bfdc926a2f37"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "if model_checkpoint in [\"t5-small\", \"t5-base\", \"t5-large\", \"t5-3b\", \"t5-11b\"]:\n",
    "    prefix = \"summarize: \"\n",
    "else:\n",
    "    prefix = \"\""
   ],
   "id": "92c58d6b0fb785fa"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "## 封装数据预处理函数",
   "id": "7b4fb7105a2b642"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "max_input_length = 1024\n",
    "max_target_length = 128\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    # 添加样本前缀\n",
    "    inputs = [prefix + doc for doc in examples[\"document\"]]\n",
    "    # 当样本长度超过max_length，由于truncation=True，所以会自动发生截断\n",
    "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n",
    "\n",
    "    # 在seq2seq模型中，source text是用于encoder，target text是用于decoder在训练时需要预测的标签\n",
    "    # 所以这里会选择把summary转成input_ids组成的labels，用于decoder预测\n",
    "    # 如果只是简单的tokenizer(examples[\"summary\"])，分词器会认为该样本只是普通数据，不知道这是训练的目标；\n",
    "    labels = tokenizer(text_target=examples[\"summary\"], max_length=max_target_length, truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "preprocess_function(raw_datasets['train'][:2])"
   ],
   "id": "c624334a2ea1cbed"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 数据映射",
   "id": "3f5f8f87fdbdcde4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)",
   "id": "237d1be3a60f1721"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 加载预训练模型",
   "id": "fd270d6bb911634"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 加载预训练模型\n",
    "from transformers import AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)"
   ],
   "id": "ef54b95e6782f369"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 封装训练参数配置器",
   "id": "72bed870983e5300"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "batch_size = 16\n",
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    f\"{model_name}-finetuned-xsum\",\n",
    "    eval_strategy = \"epoch\", # 每个epoch训练完即评估\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3, # seq2seqTrainer会定期保存模型权重，所以设置最多保存3个\n",
    "    num_train_epochs=1,\n",
    "    predict_with_generate=True,\n",
    "    fp16=True,\n",
    "    report_to=\"none\"\n",
    ")"
   ],
   "id": "ccee7adc5fe1ff18"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 获取数据加载器\n",
    "\n",
    "它会将每个batch的input进行最大长度对齐，同样也会将labels填充到最大长度；"
   ],
   "id": "117a2bcb98bc9cdd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)",
   "id": "ea3e0477357f3891"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 封装评估函数",
   "id": "ff03cb889ce59b01"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 封装评估指标计算函数\n",
    "import nltk\n",
    "import numpy as np\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    # 把labels中-100的值，替换成分词器的特殊token，pad_token，在decode时可以跳过\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # ROUGE 指标库希望输入的参考答案和预测答案是 按句子分行的文本。\n",
    "    # nltk.sent_tokenize：会把一句话切分成多个句子。例如：\"I love NLP. It is fun.\" → [\"I love NLP.\", \"It is fun.\"]\n",
    "    decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]\n",
    "    decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]\n",
    "\n",
    "    # use_stemmer=True：主要是把单词提取词干，例如：running → run，避免因为词形变化导致分数偏低。\n",
    "    # use_aggregator=True：会对整个 dataset 的结果聚合（平均），而不是逐个样本输出\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True, use_aggregator=True)\n",
    "    # 把结果转成百分制\n",
    "    result = {key: value * 100 for key, value in result.items()}\n",
    "\n",
    "    # 统计predictions里面非特殊token的数量，再取平均值\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "\n",
    "    return {k: round(v, 4) for k, v in result.items()}"
   ],
   "id": "2a23c09503c92459"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 构建训练器",
   "id": "1303ce92453eb08a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ],
   "id": "3be166a831c7e864"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 开始训练",
   "id": "866039272a6610a4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 一个epoch需要跑2个半小时\n",
    "\n",
    "trainer.train()"
   ],
   "id": "7f339c6ed0f0a6be"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
