{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 安装依赖\n",
    "\n",
    "如果当前环境已经有相关依赖了，则不用执行"
   ],
   "id": "6bb38b89961a88f"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": "!pip install transformers[torch] datasets==3.6.0 evaluate",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 加载数据",
   "id": "aa30579551cb09a9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset\n",
    "import evaluate\n",
    "\n",
    "task = \"cola\"\n",
    "model_checkpoint = \"distilbert-base-uncased\"\n",
    "batch_size = 16\n",
    "# 除了mnli-mm以外，其他任务都可以直接通过任务名字进行加载。数据加载之后会自动缓存。\n",
    "actual_task = \"mnli\" if task == \"mnli-mm\" else task\n",
    "dataset = load_dataset(\"glue\", actual_task)\n",
    "# 评估指标加载\n",
    "metric = evaluate.load(\"glue\", actual_task)"
   ],
   "id": "6d9c1cdb14a22d17",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 你可以查看dataset或metric对象都封装了什么东西\n",
    "dataset"
   ],
   "id": "252b8a6106794aba",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 数据集可视化\n",
    "\n",
    "为了能够进一步理解数据长什么样子，下面的函数将从数据集里随机选择几个例子进行展示。"
   ],
   "id": "3be7bcd460e55948"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import datasets\n",
    "import random\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def show_random_element(dataset, num_examples=10):\n",
    "  assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n",
    "  picks = []\n",
    "  for _ in range(num_examples):\n",
    "    pick = random.randint(0, len(dataset) - 1)\n",
    "    while pick in picks:\n",
    "      pick = random.randint(0, len(dataset) - 1)\n",
    "    picks.append(pick)\n",
    "\n",
    "  df = pd.DataFrame(dataset[picks])\n",
    "  for column, typ in dataset.features.items():\n",
    "    if isinstance(typ,datasets.ClassLabel):\n",
    "      df[column] = df[column].transform(lambda i: typ.names[i])\n",
    "  display(HTML(df.to_html()))\n",
    "\n",
    "show_random_element(dataset[\"train\"])"
   ],
   "id": "3dc6c4c177f2601e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 数据预处理",
   "id": "c8961c311c065f21"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "1. 获取分词器",
   "id": "3eb4459784511104"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)"
   ],
   "id": "e92aadf8b2ef7a3c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "`use_fast=True` 要求tokenizer必须是transformers.PreTrainedTokenizerFast类型；\n",
    "\n",
    "因为我们在预处理的时候需要用到fast tokenizer的一些特殊特性（比如多线程快速tokenizer，offset mapping等）。\n",
    "\n",
    "如果对应的模型没有fast tokenizer，去掉这个选项即可。\n",
    "\n",
    "几乎所有模型对应的tokenizer都有对应的fast tokenizer;可以到模型API文档中查看：https://huggingface.co/docs/transformers/v4.53.3/en/model_doc/bert#transformers.BertTokenizer"
   ],
   "id": "6153eeae1b3ab215"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "2. 根据具体的任务，处理数据集的样本",
   "id": "3094004fccc0722c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 不同的任务数据集的字段是不同的\n",
    "task_to_keys = {\n",
    "    \"cola\": (\"sentence\", None),\n",
    "    \"mnli\": (\"premise\", \"hypothesis\"),\n",
    "    \"mnli-mm\": (\"premise\", \"hypothesis\"),\n",
    "    \"mrpc\": (\"sentence1\", \"sentence2\"),\n",
    "    \"qnli\": (\"question\", \"sentence\"),\n",
    "    \"qqp\": (\"question1\", \"question2\"),\n",
    "    \"rte\": (\"sentence1\", \"sentence2\"),\n",
    "    \"sst2\": (\"sentence\", None),\n",
    "    \"stsb\": (\"sentence1\", \"sentence2\"),\n",
    "    \"wnli\": (\"sentence1\", \"sentence2\")\n",
    "}"
   ],
   "id": "67e7ac59919ee549",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "3. 对数据格式进行检查",
   "id": "c01d804c6038c8fd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "sentence1_key, sentence2_key = task_to_keys[task]\n",
    "if sentence2_key is None:\n",
    "    print(f\"Sentence: {dataset['train'][0][sentence1_key]}\")\n",
    "else:\n",
    "    print(f\"Sentence 1: {dataset['train'][0][sentence1_key]}\")\n",
    "    print(f\"Sentence 2: {dataset['train'][0][sentence2_key]}\")"
   ],
   "id": "82499eabd08eaa64",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "封装成预处理函数",
   "id": "678c05fe957d28ed"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def preprocess_function(examples):\n",
    "    if sentence2_key is None:\n",
    "        return tokenizer(examples[sentence1_key], truncation=True) # truncation 超过512自动截断\n",
    "    return tokenizer(examples[sentence1_key], examples[sentence2_key], truncation=True)"
   ],
   "id": "88fb60e655b8ec25",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "4. 对预处理后的数据集进行数据映射",
   "id": "3d67fc9b98e47eb2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# batched=True，num_proc=4；使用多线程同时并行对输入进行批处理\n",
    "encoded_dataset = dataset.map(preprocess_function, batched=True, num_proc=4)"
   ],
   "id": "7dba302e13adab23",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 加载预训练模型",
   "id": "c59a716f1f844f4d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "# 设置NLP任务中，标签的类别总数，不同任务，标签数据有所区别\n",
    "num_labels = 3 if task.startswith(\"mnli\") else 1 if task == \"stsb\" else 2\n",
    "\n",
    "# 加载预训练模型\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=num_labels)"
   ],
   "id": "c2a84e2ce8f51b31",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 获取评估指标",
   "id": "a00d9579a7be24b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "metric_name = \"pearson\" if task == \"stsb\" else \"matthews_correlation\" if task == \"cola\" else \"accuracy\"",
   "id": "cded09ffa380e539",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 创建训练参数配置类",
   "id": "213f69924dac72c9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "args = TrainingArguments(\n",
    "    \"test-glue\",                            # 自定义的任务名称\n",
    "    eval_strategy=\"epoch\",                  # 每轮都测评\n",
    "    save_strategy=\"epoch\",                  # 每轮都保存权重\n",
    "    learning_rate=2e-5,                     # 优化器的初始学习率\n",
    "    per_device_train_batch_size=batch_size, # 训练阶段的批次大小\n",
    "    per_device_eval_batch_size=batch_size,  # 验证阶段的批次大小\n",
    "    num_train_epochs=5,                     # 总训练轮次\n",
    "    weight_decay=0.01,                      # 权重衰减\n",
    "    load_best_model_at_end=True,            # 在最后阶段加载最佳模型权重\n",
    "    metric_for_best_model=metric_name,      # 评估不同权重的模型时所使用的指标\n",
    "    report_to=\"none\"                        # 是否需要同步记录训练日志\n",
    ")"
   ],
   "id": "f3f7d2cc16d131f1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 构建评估指标函数",
   "id": "575c02b3eb894950"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "# 不同的任务，测评指标不同；\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    if task != \"stsb\":\n",
    "        predictions = np.argmax(predictions, axis=1)\n",
    "    else:\n",
    "        predictions = predictions[:, 0]\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ],
   "id": "47be01b94d718994",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 创建训练器",
   "id": "b4778a66ad397369"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "validation_key = \"validation_mismatched\" if task == \"mnli-mm\" else \"validation_matched\" if task == \"mnli\" else \"validation\"\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=encoded_dataset[\"train\"],\n",
    "    eval_dataset=encoded_dataset[validation_key],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ],
   "id": "a9e95fccdb0a49d5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 开始训练",
   "id": "e5a5ce118933a41a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "trainer.train()",
   "id": "8c4742be84d48494"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 模型评估",
   "id": "d18de82f4785b68b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "trainer.evaluate()",
   "id": "c78eda204d8daf02"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
