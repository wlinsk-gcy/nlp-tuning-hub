{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 安装依赖\n",
    "\n",
    "如果当前环境已经有相关依赖了，则不用执行"
   ],
   "id": "8f37a32bab816476"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "!pip install transformers[torch] datasets==3.6.0 evaluate",
   "id": "6579dd8d976f429f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 加载数据",
   "id": "776beb8cc802fc3b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# squad_v2等于True或者False分别代表使用SQUAD v1 或者 SQUAD v2。\n",
    "# 如果您使用的是其他数据集，那么True代表的是：模型可以回答“不可回答”问题，也就是部分问题不给出答案，而False则代表所有问题必须回答。\n",
    "squad_v2 = False\n",
    "model_checkpoint = \"distilbert-base-uncased\"\n",
    "batch_size = 16\n",
    "\n",
    "datasets = load_dataset(\"squad_v2\" if squad_v2 else \"squad\")"
   ],
   "id": "34406f88928b09df",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 你可以查看dataset对象都封装了什么东西\n",
    "datasets"
   ],
   "id": "9a7401ea6dd4ff9c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 数据集可视化\n",
    "\n",
    "为了能够进一步理解数据长什么样子，下面的函数将从数据集里随机选择几个例子进行展示。"
   ],
   "id": "2427e7aedcc6f9f2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from datasets import ClassLabel, Sequence\n",
    "import random\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def show_random_elements(dataset, num_examples=10):\n",
    "  assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n",
    "  picks = []\n",
    "  for _ in range(num_examples):\n",
    "      pick = random.randint(0, len(dataset)-1)\n",
    "      while pick in picks:\n",
    "          pick = random.randint(0, len(dataset)-1)\n",
    "      picks.append(pick)\n",
    "\n",
    "\n",
    "  df = pd.DataFrame(dataset[picks])\n",
    "  for column, typ in dataset.features.items():\n",
    "    if isinstance(typ, ClassLabel):\n",
    "        df[column] = df[column].transform(lambda i: typ.names[i])\n",
    "    elif isinstance(typ, Sequence) and isinstance(typ.feature, ClassLabel):\n",
    "        df[column] = df[column].transform(lambda x: [typ.feature.names[i] for i in x])\n",
    "  display(HTML(df.to_html()))\n",
    "\n",
    "show_random_elements(datasets[\"train\"], num_examples=2)"
   ],
   "id": "f50e98264c181498",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 数据预处理",
   "id": "6470e61e97d43f8b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "print(tokenizer(\"What is your name?\"))\n",
    "print(\"单个文本tokenize: {}\".format(tokenizer.tokenize(\"What is your name?\")))\n",
    "# add_special_tokens=True：会把[CLS][SEP]等特殊token拼接上去\n",
    "print(\"单个文本tokenize: {}\".format(tokenizer.tokenize(\"What is your name?\",add_special_tokens=True)))\n",
    "# [SEP]的id是102，会把两个句子的tokenids拼一起，然后用102分隔；\n",
    "print(tokenizer(\"What is your name?\", \"My name is Sylvain.\"))"
   ],
   "id": "53a687ad54c4a318",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 当question和context序列太长怎么处理？\n",
    "\n",
    "定义两个变量，一个是最大长度，一个是重复元素个数；\n",
    "\n",
    "超过最大长度自动截断，重复元素指一个长序列被拆分后，序列与序列之间的重复元素，避免序列被分隔后语义被破坏；"
   ],
   "id": "d3f7ca7a2b11b99c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 特征的最大长度（问题和上下文）\n",
    "max_length = 384\n",
    "\n",
    "for i, example in enumerate(datasets[\"train\"]):\n",
    "  if len(tokenizer(example[\"question\"],example[\"context\"])[\"input_ids\"]) > max_length:\n",
    "    break\n",
    "\n",
    "example = datasets[\"train\"][i]\n",
    "\n",
    "len(tokenizer(example[\"question\"], example[\"context\"])[\"input_ids\"])"
   ],
   "id": "5db22776af6574cf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "> 可以看到，如果不截断的话，长度是396",
   "id": "4ff6dbea05be91cb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "如果直接按max_length的限制截断，就会损失一些内容，导致语义被破坏；",
   "id": "c21fa31f425613b0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 按比例切分后序列之间的重复长度\n",
    "doc_stride = 128\n",
    "\n",
    "tokenized_example = tokenizer(example[\"question\"],example[\"context\"],truncation=\"only_second\",max_length=max_length,stride=doc_stride,return_overflowing_tokens=True)\n",
    "\n",
    "print([len(x) for x in tokenized_example[\"input_ids\"]])"
   ],
   "id": "d54d0a408be63598",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "> runcation=\"only_second\"：表示指截断第二个序列（context）\n",
    ">\n",
    "> stride=doc_stride：表示按比例切分后序列之间的重复长度\n",
    ">\n",
    "> return_overflowing_tokens=True：表示当输入序列被截断（超过 max_length）时，返回所有溢出的 token（即被截断掉的部分），并生成多个“滑动窗口”形式的输入样本。"
   ],
   "id": "650bb00e2ef419f4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "> 假设max_length=10, stride=4,上下文tokens为：\n",
    ">\n",
    "> [CLS] Q1 Q2 [SEP] C1 C2 C3 C4 C5 C6 C7 C8 C9 C10 C11 C12 [SEP]\n",
    ">\n",
    "> 就会被切成：\n",
    ">\n",
    "> 窗口1: [CLS] Q1 Q2 [SEP] C1 C2 C3 C4 C5 C6 [SEP]\n",
    ">\n",
    "> 窗口2: [CLS] Q1 Q2 [SEP] C5 C6 C7 C8 C9 C10 [SEP]\n",
    ">\n",
    "> 窗口3: [CLS] Q1 Q2 [SEP] C9 C10 C11 C12 [SEP]"
   ],
   "id": "7d2231e988953119"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 新问题\n",
    "\n",
    "\n",
    "由于序列被切分后，溢出部分和question序列组成了新的序列对；原本数据集中标注好的answer的start_id已经对不上了；怎么处理？\n",
    "\n",
    "可以让tokenizer返回offsets_mapping，它可以返回每个token在切片后的位置和原始未切分时的位置的对应关系；"
   ],
   "id": "61bbba34d716729f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "tokenized_example = tokenizer(\n",
    "    example[\"question\"],\n",
    "    example[\"context\"],\n",
    "    max_length=max_length,\n",
    "    truncation=\"only_second\",\n",
    "    return_overflowing_tokens=True,\n",
    "    return_offsets_mapping=True,\n",
    "    stride=doc_stride\n",
    ")\n",
    "# 打印切片前后位置下标的对应关系\n",
    "tokenized_example[\"offset_mapping\"][0][:100]"
   ],
   "id": "7d70d1de4b5e612e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "> [(0, 0), (0, 3), (4, 8), (9, 13), (14, 18), (19, 22), ...., (409, 415), (416, 418)]\n",
    ">\n",
    "> 第一个token是[CLS],他不属于question和context，所以是(0,0)；\n",
    ">\n",
    "> 第二个token的起始和结束位置(0,3), 我们可以根据切片后的token id转化对应的token；然后使用offset_mapping参数映射回切片前的token位置，找到原始位置的tokens；\n",
    ">\n",
    "> 由于question拼接在context前面，所以直接从question里根据下标找就行了；"
   ],
   "id": "a40e2e84683c8a3c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "first_token_id = tokenized_example[\"input_ids\"][0][1]\n",
    "print(first_token_id) # 2129\n",
    "offsets = tokenized_example[\"offset_mapping\"][0][1]\n",
    "print(offsets) # (0, 3)\n",
    "print(tokenizer.convert_ids_to_tokens([first_token_id])[0], example[\"question\"][offsets[0]:offsets[1]]) # how How"
   ],
   "id": "e6153abe679e5e33",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "这样就可以找到切分后的token和原始token的位置对应关系，接下来只需要处理原始token是在序列1还是序列2就行，通过sequence_ids()这个api就可以拿到每个token的序列下标；",
   "id": "8a88862324d8face"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "sequence_ids = tokenized_example.sequence_ids()\n",
    "print(sequence_ids)"
   ],
   "id": "c22c0574168ec5aa",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "> [None, 0, 0,..., 0, None, 1, 1, 1, 1, ..., 1, 1, None]\n",
    ">\n",
    "> 这几个None就是[CLS]和[SEP]，因为他们不属于任何一个序列，只是用于占位，所以为None；"
   ],
   "id": "b3978ceb46b5b070"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "现在我们把上面的步骤整合起来，要找到标注样本的答案，在分割后的序列的位置；（新的位置）",
   "id": "a0f4d7fbc350a439"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "answers = example[\"answers\"]\n",
    "start_char = answers[\"answer_start\"][0]\n",
    "end_char = start_char + len(answers[\"text\"][0])\n",
    "\n",
    "# 找第二个序列context的起始和结束位置\n",
    "# 找到当前文本的start token index\n",
    "token_start_index = 0\n",
    "while sequence_ids[token_start_index] != 1:\n",
    "  token_start_index += 1\n",
    "\n",
    "# 找到当前文本的end token index\n",
    "token_end_index = len(tokenized_example[\"input_ids\"][0]) - 1\n",
    "while sequence_ids[token_end_index] != 1:\n",
    "    token_end_index -= 1\n",
    "\n",
    "# 检测答案是否在文本区间的外部，这种情况下意味着该样本的数据标注在CLS token位置。\n",
    "offsets = tokenized_example[\"offset_mapping\"][0]\n",
    "if(offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n",
    "  # 将token_start_index和token_end_index移动到answer所在位置的两侧.\n",
    "  # 注意：答案在最末尾的边界条件.\n",
    "  while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n",
    "    token_start_index += 1\n",
    "  start_position = token_start_index - 1\n",
    "  while offsets[token_end_index][1] >= end_char:\n",
    "    token_end_index -= 1\n",
    "  end_position = token_end_index + 1\n",
    "  print(\"start_position: {}, end_position: {}\".format(start_position, end_position))\n",
    "else:\n",
    "  print(\"The answer is not in this feature.\")"
   ],
   "id": "b06b54428b27b1aa",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "验证一下这个位置的token和标注样本中的answer是否一致：",
   "id": "3d750f2f9a719027"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 验证转换后的id对应的token跟答案是否准确\n",
    "print(tokenizer.decode(tokenized_example[\"input_ids\"][0][start_position: end_position+1]))\n",
    "print(answers[\"text\"][0])"
   ],
   "id": "b198bc5884d7ca81",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 新问题\n",
    "\n",
    "现在是固定了question是序列1，context是序列2；\n",
    "\n",
    "那不同模型要求不同，可能要求相反的位置，怎么解决？\n",
    "\n",
    "需要用多一个变量才处理一下:padding_side"
   ],
   "id": "31207a1f24dc35fd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "pad_on_right = tokenizer.padding_side == \"right\" #context在右边",
   "id": "e877d96e9a0d2da2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 封装完整的数据预处理函数",
   "id": "be45285df0c43aa9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def prepare_train_features(examples):\n",
    "  # 既要对examples进行truncation（截断）和padding（补全）还要还要保留所有信息，所以要用的切片的方法。\n",
    "  # 每一个超长文本example会被切片成多个输入，相邻两个输入之间会有交集。\n",
    "  tokenized_examples = tokenizer(\n",
    "      examples[\"question\" if pad_on_right else \"context\"],\n",
    "      examples[\"context\" if pad_on_right else \"question\"],\n",
    "      truncation=\"only_second\" if pad_on_right else \"only_first\",\n",
    "      max_length=max_length,\n",
    "      stride=doc_stride,\n",
    "      return_overflowing_tokens=True,\n",
    "      return_offsets_mapping=True,\n",
    "      padding=\"max_length\", # 长度不足max_length的序列会被填充到max_length长度\n",
    "  )\n",
    "  # 我们使用overflow_to_sample_mapping参数来映射切片片ID到原始ID。\n",
    "  # 比如有2个expamples被切成4片，那么对应是[0, 0, 1, 1]，前两片对应原来的第一个example。\n",
    "  sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
    "  # offset_mapping也对应4片\n",
    "  # offset_mapping参数帮助我们映射到原始输入，由于答案标注在原始输入上，所以有助于我们找到答案的起始和结束位置。\n",
    "  offset_mapping = tokenized_examples.pop(\"offset_mapping\")\n",
    "  # 重新标注数据\n",
    "  tokenized_examples[\"start_positions\"] = []\n",
    "  tokenized_examples[\"end_positions\"] = []\n",
    "\n",
    "  for i, offsets in enumerate(offset_mapping):\n",
    "    # 对每一片进行处理，将无答案的样本标注到CLS上\n",
    "    input_ids = tokenized_examples[\"input_ids\"][i]\n",
    "    cls_index = input_ids.index(tokenizer.cls_token_id)\n",
    "    # 区分question和context\n",
    "    sequence_ids = tokenized_examples.sequence_ids(i)\n",
    "    # 拿到原始的example 下标.\n",
    "    sample_index = sample_mapping[i]\n",
    "    answers = examples[\"answers\"][sample_index]\n",
    "    # 如果没有答案，则使用CLS所在的位置为答案.\n",
    "    if len(answers[\"answer_start\"]) == 0:\n",
    "      tokenized_examples[\"start_positions\"].append(cls_index)\n",
    "      tokenized_examples[\"end_positions\"].append(cls_index)\n",
    "    else:\n",
    "      # 答案的character级别Start/end位置.\n",
    "      start_char = answers[\"answer_start\"][0]\n",
    "      end_char = start_char + len(answers[\"text\"][0])\n",
    "      # 找到token级别的index start.\n",
    "      token_start_index = 0\n",
    "      while sequence_ids[token_start_index] != (1 if pad_on_right else 0):\n",
    "          token_start_index += 1\n",
    "      # 找到token级别的index end.\n",
    "      token_end_index = len(input_ids) - 1\n",
    "      while sequence_ids[token_end_index] != (1 if pad_on_right else 0):\n",
    "          token_end_index -= 1\n",
    "      # 检测答案是否超出文本长度，超出的话也用CLS index作为标注.\n",
    "      if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n",
    "        tokenized_examples[\"start_positions\"].append(cls_index)\n",
    "        tokenized_examples[\"end_positions\"].append(cls_index)\n",
    "      else:\n",
    "        # 如果不超出则找到答案token的start和end位置。.\n",
    "        # Note: we could go after the last offset if the answer is the last word (edge case).\n",
    "        while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n",
    "          token_start_index += 1\n",
    "        tokenized_examples[\"start_positions\"].append(token_start_index - 1)\n",
    "        while offsets[token_end_index][1] >= end_char:\n",
    "          token_end_index -= 1\n",
    "        tokenized_examples[\"end_positions\"].append(token_end_index + 1)\n",
    "\n",
    "  return tokenized_examples\n"
   ],
   "id": "1afc02b5bb2862d2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "同样的，调用预处理函数后，返回的是一个字典结构的数据，模型无法直接使用，需要封装成dataset对象；\n",
    "\n",
    "这里还需要remove_columns=datasets[\"train\"].column_names，把数据集中的字段列表['id', 'context', 'question', 'answers']移除掉，只把特征映射回dataset即可；\n",
    "\n",
    "**因为模型在计算loss的时候，不需要用到question，context等字段，留着占内存，所以删掉；**"
   ],
   "id": "e164a9f9f60806be"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "tokenized_datasets = datasets.map(prepare_train_features, batched=True, remove_columns=datasets[\"train\"].column_names)",
   "id": "c9ac2fb25364b658"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 加载预训练模型",
   "id": "e41085232165fb13"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from transformers import AutoModelForQuestionAnswering\n",
    "\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)"
   ],
   "id": "26e438855cee7f05"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 创建训练参数配置类",
   "id": "410538f954784e8b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "args = TrainingArguments(\n",
    "    f\"test-squad\",\n",
    "    eval_strategy = \"epoch\",\n",
    "    learning_rate=2e-5, #学习率\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=3, # 训练的论次\n",
    "    weight_decay=0.01,\n",
    "    report_to=\"none\"  # 关闭wandb等\n",
    ")"
   ],
   "id": "452402d4a76eccf2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 创建训练器",
   "id": "572339f7f57cdea5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from transformers import Trainer, default_data_collator\n",
    "\n",
    "data_collator = default_data_collator\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ],
   "id": "cd6cf3115aa1d41"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 开始训练",
   "id": "5a7307fdd1d034d1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "trainer.train()",
   "id": "6a4e54d9b73be1d4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 由于训练的时长太长，一个epoch要两个小时，所以保存一下权重\n",
    "trainer.save_model(\"test-squad-trained\") # 保存训练权重"
   ],
   "id": "c7773113ae208917"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 模型评估（较复杂）\n",
    "\n",
    "评估需要做一个处理，模型本身是预测的answer所在位置的概率，例如：\n",
    "> 答案起始位置的概率：(tensor([ 46,  57,  78,  43, 118,  15,  72,  35,  15,  34,  73,  41,  80,  91,  156,  35], device='cuda:0')\n",
    ">\n",
    "> 答案结束位置的概率： tensor([ 47,  58,  81,  55, 118, 110,  75,  37, 110,  36,  76,  53,  83,  94,  158,  35], device='cuda:0'))\n",
    "\n",
    "正常我们还需要考虑，找不到答案的情况，或者答案的位置下标不合法，或者答案的位置指向了question，这都是不行的；\n",
    "\n",
    "这种情况一个比较好处理的就是，分数最高的位置如何不合法，那取第二高？第三高？\n",
    "\n",
    "所以我们会用一个变量来限制取前n_best_size高的分数，过滤掉不合法的位置后，再排序，取第一个就可以了；\n",
    "\n",
    "例如：\n",
    "> [\n",
    "> {'score': 16.706663, 'text': 'Denver Broncos'},\n",
    "> {'score': 14.635585,'text': 'Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers'},\n",
    "> {'score': 13.234194, 'text': 'Carolina Panthers'},\n",
    "> {'score': 12.468662, 'text': 'Broncos'},\n",
    "> {'score': 11.709289, 'text': 'Denver'}\n",
    "> ]\n",
    "\n",
    "如果要检查答案位置的文本是否在context里，需要对验证数据集的特征集进行处理；\n",
    "\n",
    "一个样本example会对应多个feature，可能因为过长被切分成多个；\n",
    "\n",
    "以及每个原始token的位置与切分后token的位置对应关系：offset_mapping;"
   ],
   "id": "deb793b5861c855e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 验证数据集的预处理",
   "id": "91fbaef609436ba"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 对验证集样本进行特征预处理\n",
    "def prepare_validation_features(examples):\n",
    "    tokenized_examples = tokenizer(\n",
    "        examples[\"question\" if pad_on_right else \"context\"],\n",
    "        examples[\"context\" if pad_on_right else \"question\"],\n",
    "        truncation=\"only_second\" if pad_on_right else \"only_first\",\n",
    "        max_length=max_length,\n",
    "        stride=doc_stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "    # 移除掉overflow_to_sample_mapping字段\n",
    "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
    "\n",
    "    # 添加一个字段\n",
    "    tokenized_examples[\"example_id\"] = []\n",
    "\n",
    "    # 遍历每个样本里的每个特征集\n",
    "    for i in range(len(tokenized_examples[\"input_ids\"])):\n",
    "        # 拿到序列id,是属于第一个序列还是第二个\n",
    "        sequence_ids = tokenized_examples.sequence_ids(i)\n",
    "        context_index = 1 if pad_on_right else 0\n",
    "\n",
    "        # 获取对应序列id对应的所有特征集\n",
    "        sample_index = sample_mapping[i]\n",
    "        # tokenized_examples[\"example_id\"] = ['abc', 'abc', 'def'] 就是每个feature对应的原始样本ID\n",
    "        tokenized_examples[\"example_id\"].append(examples[\"id\"][sample_index])\n",
    "\n",
    "        # 将非 context 的 token offset 置为 None（比如 [CLS], question 部分），方便后处理时只关注 context 位置\n",
    "        # 例如原始 offset_mapping:[(0,0), (0,5), (6,8), (9,12), (13,19), None, (0,3), (4,9)]\n",
    "        # 清理后:[None, None, None, None, None, None, (0,3), (4,9)]  # 仅保留context\n",
    "        tokenized_examples[\"offset_mapping\"][i] = [\n",
    "            (o if sequence_ids[k] == context_index else None)\n",
    "            for k, o in enumerate(tokenized_examples[\"offset_mapping\"][i])\n",
    "        ]\n",
    "\n",
    "    return tokenized_examples\n",
    "\n",
    "validation_features = datasets[\"validation\"].map(\n",
    "    prepare_validation_features,\n",
    "    batched=True,\n",
    "    remove_columns=datasets[\"validation\"].column_names # 删掉原始的字段,不要占内存\n",
    ")\n",
    "# 获得所有预测结果 (start_logits, end_logits)\n",
    "raw_predictions = trainer.predict(validation_features)"
   ],
   "id": "2cb626e417a89272"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 预处理函数的代码详解",
   "id": "7f2c3eb95f1c2bb3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "examples = validation_features\n",
    "tokenized_examples = tokenizer(\n",
    "        examples[\"question\" if pad_on_right else \"context\"],\n",
    "        examples[\"context\" if pad_on_right else \"question\"],\n",
    "        truncation=\"only_second\" if pad_on_right else \"only_first\",\n",
    "        max_length=max_length,\n",
    "        stride=doc_stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )"
   ],
   "id": "e881d5ff68fc6eb4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "处理后tokenized_examples的内部有三个属性，其数据样本为：\n",
    "\n",
    "> tokenized_examples[\"input_ids\"][0][:10]\n",
    ">\n",
    "> input_ids的输出：[101, 2073, 2003, 1996, 9440, 3305, 102, 1996, 9440, 3305]\n",
    ">\n",
    "> 特征对应的token为： [CLS] where is the eiffel tower ? [SEP] the eiffel tower...\n",
    ">\n",
    "> tokenized_examples[\"attention_mask\"][0][:10]\n",
    ">\n",
    "> attention_mask的输出：[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
    ">\n",
    "> tokenized_examples[\"offset_mapping\"][0][:5]\n",
    ">\n",
    "> offset_mapping的输出为：[(0, 0), (0, 5), (6, 8), (9, 12), (13, 19)]   # token->原文字符区间"
   ],
   "id": "1b1791e785843a52"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")",
   "id": "8e9c720cb5b9f077"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "因为长 context 会被分割成多个 feature，这里记录每个 feature 对应哪个原始样本。\n",
    "\n",
    "`.pop` 会从 `tokenized_examples` 中移除该字段。\n",
    "\n",
    "sample_mapping = [0, 0, 1]  表示前两个 feature 来自第0号样本，最后一个来自第1号样本。"
   ],
   "id": "e220ff695cf80ee9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 验证数据集后置处理函数",
   "id": "5e84f4d9de75c300"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import collections\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "\n",
    "def postprocess_qa_predictions(examples, features, raw_predictions, tokenizer, n_best_size=20, max_answer_length=30):\n",
    "    all_start_logits, all_end_logits = raw_predictions\n",
    "    # 把原本的id list转成 kv形式方便定位每个example\n",
    "    example_id_to_index = {k: i for i, k in enumerate(examples[\"id\"])}\n",
    "    # 用来存储每个 example 对应的 feature 索引（因为长文本会被切成多个特征）。\n",
    "    features_per_example = collections.defaultdict(list)\n",
    "    # 一个example可能对应多个特征集，所以要把多个特征集都归到同一个example下\n",
    "    for i, feature in enumerate(features):\n",
    "        features_per_example[example_id_to_index[feature[\"example_id\"]]].append(i)\n",
    "\n",
    "    # 有序字典，用于存储每个样本的预测答案\n",
    "    predictions = collections.OrderedDict()\n",
    "    print(f\"Post-processing {len(examples)} examples split into {len(features)} features.\")\n",
    "\n",
    "    for example_index in tqdm(range(len(examples))):\n",
    "        example = examples[example_index]\n",
    "        # 获取当前样本的特征集\n",
    "        feature_indices = features_per_example[example_index]\n",
    "        # 如果是 squad_v2，用于记录预测为 \"无答案\" 的得分（CLS token 分数）\n",
    "        min_null_score = None\n",
    "        # 保存当前样本的所有候选答案\n",
    "        valid_answers = []\n",
    "        context = example[\"context\"]\n",
    "\n",
    "        for feature_index in feature_indices:\n",
    "            # 取出该 feature 对应的预测分数和 offset_mapping（token 到原文字符位置的映射）\n",
    "            start_logits = all_start_logits[feature_index]\n",
    "            end_logits = all_end_logits[feature_index]\n",
    "            # 分词后的token位置和原始token的对应位置\n",
    "            offset_mapping = features[feature_index][\"offset_mapping\"]\n",
    "\n",
    "            # 找出 [CLS] 位置（模型用它表示无答案）\n",
    "            cls_index = features[feature_index][\"input_ids\"].index(tokenizer.cls_token_id)\n",
    "            # 计算无答案得分（CLS 起点 + CLS 终点）\n",
    "            feature_null_score = start_logits[cls_index] + end_logits[cls_index]\n",
    "            # 找当前样本所有切片中的最低无答案得分\n",
    "            if min_null_score is None or feature_null_score < min_null_score:\n",
    "                min_null_score = feature_null_score\n",
    "\n",
    "            # 去分数最高的n_best_size个起点/终点，例如：\n",
    "            start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
    "            end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
    "\n",
    "            # 这里是双层for遍历所有组合\n",
    "            for start_index in start_indexes:\n",
    "                for end_index in end_indexes:\n",
    "                    # 超出上下文范围、offset 为 None、终点在起点前、答案长度超限的，都不要\n",
    "                    if (\n",
    "                        start_index >= len(offset_mapping)\n",
    "                        or end_index >= len(offset_mapping)\n",
    "                        or offset_mapping[start_index] is None\n",
    "                        or offset_mapping[end_index] is None\n",
    "                    ):\n",
    "                        continue\n",
    "                    if end_index < start_index or end_index - start_index + 1 > max_answer_length:\n",
    "                        continue\n",
    "\n",
    "                    start_char = offset_mapping[start_index][0]\n",
    "                    end_char = offset_mapping[end_index][1]\n",
    "                    # 记录合法的答案\n",
    "                    valid_answers.append({\n",
    "                        \"score\": start_logits[start_index] + end_logits[end_index],\n",
    "                        \"text\": context[start_char:end_char],\n",
    "                    })\n",
    "        # 排序后取出最高分的答案\n",
    "        best_answer = sorted(valid_answers, key=lambda x: x[\"score\"], reverse=True)[0] if valid_answers else {\"text\": \"\", \"score\": 0.0}\n",
    "        if not squad_v2:\n",
    "            predictions[example[\"id\"]] = best_answer[\"text\"]\n",
    "        else:\n",
    "            predictions[example[\"id\"]] = best_answer[\"text\"] if best_answer[\"score\"] > min_null_score else \"\"\n",
    "\n",
    "    return predictions\n",
    "\n",
    "\n",
    "final_predictions = postprocess_qa_predictions(datasets[\"validation\"], validation_features, raw_predictions.predictions)"
   ],
   "id": "9f067bc07dbad0fa"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 后置处理函数的代码详解",
   "id": "52c915aff7034d43"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "all_start_logits, all_end_logits = raw_predictions",
   "id": "3000401609c8108"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "> 这里的raw_predictions是原始的预测logits:\n",
    ">\n",
    "> `all_start_logits[i][j]` 表示第 i 个特征的第 j 个 token 是起始词的概率分数\n",
    ">\n",
    "> 例如: all_start_logits[0][:5] 输出:[-3.1, 1.2, 0.8, -0.5, 2.3]"
   ],
   "id": "97bdb5c6fd0c1f51"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "example_id_to_index = {k: i for i, k in enumerate(examples[\"id\"])}",
   "id": "4c5b5a31659ad687"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "> 这是为了建立样本 ID 到索引的映射，方便快速定位每个 example;\n",
    ">\n",
    "> 假设:examples[\"id\"] = [\"abc\", \"def\"]\n",
    ">\n",
    "> 那么example_id_to_index 就会输出: {\"abc\": 0, \"def\": 1}"
   ],
   "id": "7c265bec305d38ba"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "features_per_example = collections.defaultdict(list)",
   "id": "e955a2460100f7bb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "> 用来存储每个 example 对应的 feature 索引（因为长文本会被切成多个特征）。",
   "id": "5bb2939c6d185a12"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "features = validation_features\n",
    "\n",
    "for i, feature in enumerate(features):\n",
    "        features_per_example[example_id_to_index[feature[\"example_id\"]]].append(i)"
   ],
   "id": "87107356ffe4a9ad"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "> 一个example可能对应多个特征集，所以要把多个特征集都归到同一个example下;\n",
    ">\n",
    "> 假设: 处理后 features_per_example = {0: [0, 1],  1: [2]}\n",
    ">\n",
    "> 说明 example[0] 对应 features[0] 和 features[1] ; example[1] 就对应 features[2]; 以此类推.."
   ],
   "id": "81e47c21671a2557"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 这个是有序字典，用于存储每个样本的预测答案。\n",
    "predictions = collections.OrderedDict()"
   ],
   "id": "4b3166113408f35a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 找出 [CLS] 位置（模型用它表示无答案）\n",
    "cls_index = features[feature_index][\"input_ids\"].index(tokenizer.cls_token_id)\n",
    "# 计算无答案得分（CLS 起点 + CLS 终点）\n",
    "feature_null_score = start_logits[cls_index] + end_logits[cls_index]\n",
    "# 找当前样本所有切片中的最低无答案得分\n",
    "if min_null_score is None or feature_null_score < min_null_score:\n",
    "    min_null_score = feature_null_score"
   ],
   "id": "7a83c52ab58c3d92"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "> 这里主要需要计算以下最低的分数，当我们用的squad_v2的数据集，需要获取过滤掉小于最小分数的值；因为squad_v2的数据集是允许存在无答案的情况的；无答案一般是通过[CLS]位置的分数来作为概率；",
   "id": "d2df8172a239c2fc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 去分数最高的n_best_size个起点/终点\n",
    "# 比如n_best_size为3：那么start_indexes = [3, 2, 5]\n",
    "start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
    "end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
    "\n",
    "# 这里是双层for遍历所有组合\n",
    "for start_index in start_indexes:\n",
    "    for end_index in end_indexes:\n",
    "        # 超出上下文范围、offset 为 None、终点在起点前、答案长度超限的，都不要\n",
    "        if (\n",
    "            start_index >= len(offset_mapping)\n",
    "            or end_index >= len(offset_mapping)\n",
    "            or offset_mapping[start_index] is None\n",
    "            or offset_mapping[end_index] is None\n",
    "        ):\n",
    "            continue\n",
    "        if end_index < start_index or end_index - start_index + 1 > max_answer_length:\n",
    "            continue\n",
    "\n",
    "        start_char = offset_mapping[start_index][0]\n",
    "        end_char = offset_mapping[end_index][1]\n",
    "        # 记录合法的答案\n",
    "        valid_answers.append({\n",
    "            \"score\": start_logits[start_index] + end_logits[end_index],\n",
    "            \"text\": context[start_char:end_char],\n",
    "        })\n",
    "# 排序后取出最高分的答案\n",
    "best_answer = sorted(valid_answers, key=lambda x: x[\"score\"], reverse=True)[0] if valid_answers else {\"text\": \"\", \"score\": 0.0}"
   ],
   "id": "919a979aad352ff5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "> 这段代码主要是取原始n个最高分，然后过滤掉位置不合法的答案后，再做排序，取最大值；",
   "id": "87dcbdc64beae991"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 完整代码，请看train.py",
   "id": "88a6bebdd62c8666"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
