{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 安装依赖\n",
    "\n",
    "如果当前环境已经有相关依赖了，则不用执行"
   ],
   "id": "ea9de5d7ededcd9f"
  },
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "code",
   "source": "!pip install transformers[torch] datasets==3.6.0 evaluate seqeval",
   "id": "a5f1a50d1ea79359",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 加载数据",
   "id": "fc347763fac9bcdd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "datasets = load_dataset(\"conll2003\")"
   ],
   "id": "a04cbfb988f164d2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 你可以查看dataset对象都封装了什么东西\n",
    "datasets"
   ],
   "id": "ae3473faaeb1dd7d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 数据集可视化\n",
    "\n",
    "为了能够进一步理解数据长什么样子，下面的函数将从数据集里随机选择几个例子进行展示；"
   ],
   "id": "7e7ed69d4c9d2af2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from datasets import ClassLabel, Sequence\n",
    "import random\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def show_random_elements(dataset, num_examples=10):\n",
    "    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n",
    "    picks = []\n",
    "    for _ in range(num_examples):\n",
    "        pick = random.randint(0, len(dataset)-1)\n",
    "        while pick in picks:\n",
    "            pick = random.randint(0, len(dataset)-1)\n",
    "        picks.append(pick)\n",
    "\n",
    "    df = pd.DataFrame(dataset[picks])\n",
    "    for column, typ in dataset.features.items():\n",
    "        if isinstance(typ, ClassLabel):\n",
    "            df[column] = df[column].transform(lambda i: typ.names[i])\n",
    "        elif isinstance(typ, Sequence) and isinstance(typ.feature, ClassLabel):\n",
    "            df[column] = df[column].transform(lambda x: [typ.feature.names[i] for i in x])\n",
    "    display(HTML(df.to_html()))\n",
    "\n",
    "show_random_elements(datasets[\"train\"])"
   ],
   "id": "cfd96a7365eab21d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 数据预处理",
   "id": "b175bad76cfe93b4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "transformer预训练模型在预训练的时候通常使用的是subword(子词)；\n",
    "\n",
    "如果我们的文本输入已经被切分成了word，那么这些word还会被我们的tokenizer继续切分；就是给字词拼接上特殊Token，如（[CLS]）；根据时态语态再次划分等；\n",
    "\n",
    "例如："
   ],
   "id": "8e5b0910f75481e9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "task = \"ner\" #需要是\"ner\", \"pos\" 或者 \"chunk\"\n",
    "model_checkpoint = \"distilbert-base-uncased\"\n",
    "batch_size = 16\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "tokenized_input = tokenizer(datasets[\"train\"][4][\"tokens\"], is_split_into_words=True)\n",
    "tokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\n",
    "tokens"
   ],
   "id": "4816105e628ee12d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "`is_split_into_words=True` 的作用是告诉 tokenizer，不要把它当作一整句字符串去重新用空格切词，因为他已经是分割好的子词列表了，而是逐个 token 处理，然后再做 subword 拆分；",
   "id": "cab4e8e19d0a5fca"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "由于tokenizer将数据集里面的子词进一步分词后，tags的长度和subword的长度就对不上了；所以需要进行数据对齐；",
   "id": "d8dd8cf24dc847ce"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "len(datasets[\"train\"][4][f\"{task}_tags\"]), len(tokenized_input[\"input_ids\"])",
   "id": "774317c249726762"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 数据对齐\n",
    "\n",
    "对齐label的两种方式：\n",
    "- 多个subtokens对齐一个word，对齐一个label\n",
    "- 多个subtokens的第一个subtoken对齐word，对齐一个label，其他subtokens直接赋予-100；"
   ],
   "id": "c03f54784c8dfe99"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "word_ids = tokenized_input.word_ids()\n",
    "aligned_labels = [-100 if i is None else datasets[\"train\"][4][f\"{task}_tags\"][i] for i in word_ids]\n",
    "print(len(aligned_labels), len(tokenized_input[\"input_ids\"]))"
   ],
   "id": "a2a1645d8145a74a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "word_ids 这个API会返回每个subwords对应的wordId，因为一个词也会被拆成多个词；\n",
    "\n",
    "只需要将wordId对不上的元素，自定义处理一下，比如换成-100；像一下特殊的Token[CLS][SEP]等就是对不上的；就替换成-100，\n",
    "\n",
    "这样再计算loss时，可以自动忽略掉这些特殊Token；"
   ],
   "id": "35b8c053949db07f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 封装数据预处理函数",
   "id": "e9a1d3e49e3598bf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "label_all_tokens = True\n",
    "\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[f\"{task}_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            # 把特殊token的id设置成-100，在训练阶段计算损失值时会自动忽略\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            # 我们为每个单词的第一个token设置标签。\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label[word_idx])\n",
    "\n",
    "            # 对于单词中的其他token，我们将标签设置为当前标签或 -100，具体取决于label_all_tokens的配置。\n",
    "            else:\n",
    "                label_ids.append(label[word_idx] if label_all_tokens else -100)\n",
    "            previous_word_idx = word_idx\n",
    "\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "tokenize_and_align_labels(datasets['train'][:5])"
   ],
   "id": "7370be5ab14389f7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 数据映射",
   "id": "9a428cbada8e686"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "tokenized_datasets = datasets.map(tokenize_and_align_labels, batched=True)",
   "id": "261d3bbd3448f863"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 加载预训练模型",
   "id": "4b388d944d9912ec"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from transformers import AutoModelForTokenClassification\n",
    "\n",
    "label_list = datasets[\"train\"].features[f\"{task}_tags\"].feature.names\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_checkpoint, num_labels=len(label_list))"
   ],
   "id": "196a0e2e316ad510"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 创建训练参数配置器",
   "id": "3e6fa7a1f1b94e76"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "args = TrainingArguments(\n",
    "    f\"test-{task}\",\n",
    "    eval_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    report_to=\"none\"\n",
    ")"
   ],
   "id": "a5d8e86b208f13b1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 获取数据加载器",
   "id": "a0b12ea9226554ea"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)"
   ],
   "id": "99e41ba559c44e96"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 获取评估指标",
   "id": "b0186e935fff7441"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"seqeval\")"
   ],
   "id": "81a7aef163198b6d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 封装评估函数",
   "id": "cff219f1cd2f18da"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 融合多个指标的评估函数\n",
    "import numpy as np\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    # Remove ignored index (special tokens)\n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }"
   ],
   "id": "7888a6375ff513d8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 构建训练器",
   "id": "eafc2e53811a53ad"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ],
   "id": "13132b03e5b6d2f9"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 开始训练",
   "id": "bc9dbaa1f9872801"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "trainer.train()",
   "id": "6459708b96ea929f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 模型评估",
   "id": "ea97f27e1f48aad5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "trainer.evaluate()",
   "id": "6ae572d71d4f755a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 模型预测\n",
    "\n",
    "为了在完成训练后计算出每个类别的准确率/召回率/f1，我们可以对 predict 方法的结果应用与之前相同的函数："
   ],
   "id": "733d769dc0dbe9f9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "predictions, labels, _ = trainer.predict(tokenized_datasets[\"validation\"])\n",
    "predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "# Remove ignored index (special tokens)\n",
    "true_predictions = [\n",
    "    [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "    for prediction, label in zip(predictions, labels)\n",
    "]\n",
    "true_labels = [\n",
    "    [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "    for prediction, label in zip(predictions, labels)\n",
    "]\n",
    "\n",
    "results = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "results"
   ],
   "id": "d0f50a2712a24d3e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
